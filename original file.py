# -*- coding: utf-8 -*-
"""Anxiety_Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vwoh8mKhNQ71asjG_02Xn2l3W4ZQktJD
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pandas.core.describe import DataFrameDescriber

from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE, ADASYN
from sklearn.impute import SimpleImputer, KNNImputer
from imblearn.over_sampling import SMOTE
from sklearn.feature_selection import RFECV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import warnings

warnings.filterwarnings('ignore')

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

"""# Data Exploration"""

from google.colab import drive

drive.mount('/content/drive/')

df = pd.read_csv('./drive/Shareddrives/DATA MINING 240/Dataset_Depression/student.csv')

# Set max_rows option to None to display all rows
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

df.head()

# rows and columns in data
print("Total Rows: %s and Columns: %s" % (df.shape[0], df.shape[1]))

# checking data types of recorded data
df.dtypes

print("---------------------------DATA--------------------------------\n")
print("Nan values in dataset are: %s" % len(df[df.isna().any(1)]))
print('-' * 100)


# print('Total duplicated values in data are: %s'%(len(train_df)-len(train_df.drop_duplicates(keep='first'))))

# Check remaining missing values if any
def display_only_missing(df):
    all_data_na = (df.isnull().sum() / len(df)) * 100
    all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)
    missing_data = pd.DataFrame({'Missing Ratio': all_data_na})
    print(missing_data)


display_only_missing(df)

df.isnull().sum()

feat_columns = df.columns[df.notna().all()].tolist()
selected_features = df[feat_columns]

# select all parameters with missing data
missing_data_columns = df.columns[df.isna().any()].tolist()
missing_data = df[missing_data_columns]

missing_data.head()

# visualizing nan data in all columns
output = missing_data.isnull().sum()
plt.figure(figsize=(9, 12))
sns.barplot(x=output.values, y=output.keys())
plt.show()

"""## Encoding the columns"""

'''
Parameters with object values are filtered to replace their string
values with numeric equalent through Label encoder.
Dictionary named "dummy_encoder" stores instances of the LabelEncoder class for each object type column
in a DataFrame named "selected".
For each object column, the code initializes a LabelEncoder object named "le", and then applies the fit_transform
method to the column after converting it to a string data type using the "astype" method. This encodes each
unique value in the column as a numerical label, which can be used as input to machine learning models.

The encoded values are stored in the same column in "selected". Finally, the LabelEncoder instance is added to
the "dummy_encoder" dictionary using the column name as the key. This allows for easy reference to the LabelEncoder
instances later on, if needed.
'''

# for features
features_obj_columns = list(selected_features.select_dtypes(include='object').columns)
features_encoder = {}
for col in features_obj_columns:
    le = LabelEncoder()
    selected_features.loc[:, col] = le.fit_transform(selected_features[col].astype(str))
    features_encoder[col] = le

# for missing data
# because label encoder will encode even nan value, so to preserve it
# first we filter out nan data, then we train encoder by skipping nan rows,
# then create copy of that column and transform str data by skipping nan rows
missing_obj_columns = list(missing_data.select_dtypes(include='object').columns)
missing_encoder = {}
for col in missing_obj_columns:
    le = LabelEncoder()
    nan_mask = missing_data[col].isna()
    le.fit_transform(missing_data[col][~nan_mask])
    col_copy = missing_data[col].copy()
    col_copy.loc[~nan_mask] = le.transform(missing_data[col][~nan_mask])
    missing_data.loc[:, col] = col_copy.astype('float64')
    missing_encoder[col] = le

selected_features.head()

missing_data.head()

# create new instance of feature(dummy df) with target containing missing data
# because we have 21 targets, so creating 21 impute data arrays
living_conditions = np.append(selected_features.values,
                              missing_data['Satisfied with living conditions'].values.reshape(-1,
                                                                                              1),
                              axis=1)
parental_data = np.append(selected_features.values,
                          missing_data['Parental home'].values.reshape(-1, 1), axis=1)
pet_data = np.append(selected_features.values,
                     missing_data['Having only one parent'].values.reshape(-1, 1), axis=1)
commute_data = np.append(selected_features.values,
                         missing_data['Long commute'].values.reshape(-1, 1), axis=1)
transportation_data = np.append(selected_features.values,
                                missing_data['Mode of transportation'].values.reshape(-1, 1),
                                axis=1)
insurance_data = np.append(selected_features.values,
                           missing_data['Private health insurance '].values.reshape(-1, 1), axis=1)
weight_data = np.append(selected_features.values, missing_data['Weight (kg)'].values.reshape(-1, 1),
                        axis=1)
height_data = np.append(selected_features.values, missing_data['Height (cm)'].values.reshape(-1, 1),
                        axis=1)
obesity_data = np.append(selected_features.values,
                         missing_data['Overweight and obesity'].values.reshape(-1, 1), axis=1)
systolic_data = np.append(selected_features.values,
                          missing_data['Systolic blood pressure (mmHg)'].values.reshape(-1, 1),
                          axis=1)
diastolic_data = np.append(selected_features.values,
                           missing_data['Diastolic blood pressure (mmHg)'].values.reshape(-1, 1),
                           axis=1)
hypertension_data = np.append(selected_features.values,
                              missing_data['Prehypertension or hypertension'].values.reshape(-1, 1),
                              axis=1)
heartrate_data = np.append(selected_features.values,
                           missing_data['Heart rate (bpm)'].values.reshape(-1, 1), axis=1)
abnormal_hr_data = np.append(selected_features.values,
                             missing_data['Abnormal heart rate'].values.reshape(-1, 1), axis=1)
vaccination_data = np.append(selected_features.values,
                             missing_data['Vaccination up to date'].values.reshape(-1, 1), axis=1)
smoke_lvl5_data = np.append(selected_features.values,
                            missing_data['Cigarette smoker (5 levels)'].values.reshape(-1, 1),
                            axis=1)
smoke_lvl3_data = np.append(selected_features.values,
                            missing_data['Cigarette smoker (3 levels)'].values.reshape(-1, 1),
                            axis=1)
drinker_lvl3_data = np.append(selected_features.values,
                              missing_data['Drinker (3 levels)'].values.reshape(-1, 1), axis=1)
drinker_lvl2_data = np.append(selected_features.values,
                              missing_data['Drinker (2 levels)'].values.reshape(-1, 1), axis=1)
marijuana_data = np.append(selected_features.values,
                           missing_data['Marijuana use'].values.reshape(-1, 1), axis=1)
drugs_data = np.append(selected_features.values,
                       missing_data['Other recreational drugs'].values.reshape(-1, 1), axis=1)

"""# Preprocessing of MICE"""

!pip
install
impyute

from impyute.imputation.cs import mice

# mice imputing data
mice_imputed = mice(missing_data.values)
missing_data = pd.DataFrame(mice_imputed, columns=missing_data.columns)

# round values for int based parameters
missing_data.loc[:, 'Satisfied with living conditions'] = missing_data.loc[:,
                                                          'Satisfied with living conditions'].round().astype(
    'int')
missing_data.loc[:, 'Parental home'] = missing_data.loc[:, 'Parental home'].round().astype('int')
missing_data.loc[:, 'Having only one parent'] = missing_data.loc[:,
                                                'Having only one parent'].round().astype('int')
missing_data.loc[:, 'Long commute'] = missing_data.loc[:, 'Long commute'].round().astype('int')
missing_data.loc[:, 'Mode of transportation'] = missing_data.loc[:,
                                                'Mode of transportation'].round().astype('int')
missing_data.loc[:, 'Private health insurance '] = missing_data.loc[:,
                                                   'Private health insurance '].round().astype(
    'int')
missing_data.loc[:, 'Overweight and obesity'] = missing_data.loc[:,
                                                'Overweight and obesity'].round().astype('int')
missing_data.loc[:, 'Prehypertension or hypertension'] = missing_data.loc[:,
                                                         'Prehypertension or hypertension'].round().astype(
    'int')
missing_data.loc[:, 'Abnormal heart rate'] = missing_data.loc[:,
                                             'Abnormal heart rate'].round().astype('int')
missing_data.loc[:, 'Vaccination up to date'] = missing_data.loc[:,
                                                'Vaccination up to date'].round().astype('int')
missing_data.loc[:, 'Cigarette smoker (5 levels)'] = missing_data.loc[:,
                                                     'Cigarette smoker (5 levels)'].round().astype(
    'int')
missing_data.loc[:, 'Cigarette smoker (3 levels)'] = missing_data.loc[:,
                                                     'Cigarette smoker (3 levels)'].round().astype(
    'int')
missing_data.loc[:, 'Drinker (3 levels)'] = missing_data.loc[:,
                                            'Drinker (3 levels)'].round().astype('int')
missing_data.loc[:, 'Drinker (2 levels)'] = missing_data.loc[:,
                                            'Drinker (2 levels)'].round().astype('int')
missing_data.loc[:, 'Marijuana use'] = missing_data.loc[:, 'Marijuana use'].round().astype('int')
missing_data.loc[:, 'Other recreational drugs'] = missing_data.loc[:,
                                                  'Other recreational drugs'].round().astype('int')
# missing_data.head()

"""# Feature Forming

For BMI: bmi = weight/height*height

for Mean Arterial Pressure: MAP = DP + 0.412 (SP - DP)
DP = Dystolic Pressure
SP = Systolic Pressure

For Pulse Pressure: PP = SP - DP
"""

'''
Create new df for features which includes 3 features measured from processed features as in paper
'''

features = pd.DataFrame()

features['BMI'] = missing_data['Weight (kg)'] / (missing_data['Height (cm)'] * 0.01 * 2)

features['MAP'] = missing_data['Diastolic blood pressure (mmHg)'] + \
                  (0.412 * (missing_data['Systolic blood pressure (mmHg)'] - missing_data[
                      'Diastolic blood pressure (mmHg)']) * \
                   missing_data['Diastolic blood pressure (mmHg)'])

features['PP'] = missing_data['Systolic blood pressure (mmHg)'] - missing_data[
    'Diastolic blood pressure (mmHg)']
features.head()

# merge all features together
features = features.join(selected_features)
features = features.join(missing_data)
features = features.drop(labels=['Weight (kg)',
                                 'Height (cm)',
                                 'Systolic blood pressure (mmHg)',
                                 'Diastolic blood pressure (mmHg)',
                                 'Anxiety symptoms', 'Depressive symptoms'], axis=1)
features.head()

feature_col = features.columns
save_feat = pd.DataFrame()
for col in feature_col:
    if col in features_obj_columns:
        save_feat[col] = features_encoder[col].inverse_transform(features[col])
    elif col in missing_obj_columns:
        save_feat[col] = missing_encoder[col].inverse_transform(features[col])
    else:
        save_feat[col] = features[col]

save_feat.to_csv('MICE_imputed.csv')

"""# Normalization"""

X = features.copy()
anxiety = df['Anxiety symptoms']

scaler = MinMaxScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# X.head(2)

"""# Oversampling"""

smote_anxiety = SMOTE(random_state=42)
X_anxiety, anxiety_os = smote_anxiety.fit_resample(X, anxiety)

print('New shape:', X_anxiety.shape)

anxiety_os.value_counts().plot(kind='bar')
plt.title('Oversampled Anxiety')
plt.show()

# X_anxiety = X
# anxiety_os = anxiety
# from sklearn import preprocessing

# # label_encoder object knows
# # how to understand word labels.
# label_encoder = preprocessing.LabelEncoder()

# # Encode labels in column 'species'.
# anxiety_os = label_encoder.fit_transform(anxiety)

X_anxiety.head()

"""# Feature Selection"""

from sklearn import preprocessing

# label_encoder object knows
# how to understand word labels.
label_encoder = preprocessing.LabelEncoder()

# Encode labels in column 'species'.
anxiety_os = label_encoder.fit_transform(anxiety_os)

# Model = RandomForestClassifier(random_state=100,n_jobs=-1)

# rfecv = RFECV(estimator=Model, step=1, cv=5, scoring='recall')
# rfecv.fit(X_anxiety, anxiety_os)

# print("Optimal number of features: %d" % rfecv.n_features_)
# print("Selected features: ", X_anxiety.columns[rfecv.support_])

# dim_red = X_anxiety.columns[rfecv.support_].tolist()

# dim_red = dim_red[:20]

# X_anxiety[dim_red].head(1)

"""# Modeling

## **ANXIETY**
"""

from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier

# finding most important features for anxiety
dt = DecisionTreeClassifier()
dt.fit(X_anxiety, anxiety_os)

rf = RandomForestClassifier(n_estimators=500)
rf.fit(X_anxiety, anxiety_os)

gb = GradientBoostingClassifier()
gb.fit(X_anxiety, anxiety_os)

# finding important features from trained model
imp_feat_anx = pd.DataFrame(X_anxiety.columns, columns=['features'])
imp_feat_anx['DT_score'] = dt.feature_importances_
imp_feat_anx['RF_score'] = rf.feature_importances_
imp_feat_anx['GB_score'] = gb.feature_importances_
imp_feat_anx = imp_feat_anx.sort_values(by=['DT_score', 'RF_score', 'GB_score'], ascending=False)
imp_feat_anx.head(10)

"""**SELECTING TOP 20 FEATURES**"""

x_anx_train, x_anx_test, y_anx_train, y_anx_test = train_test_split(
    X_anxiety[imp_feat_anx['features'].head(20).values],
    anxiety_os,
    test_size=0.2,
    random_state=42,
    shuffle=True)

# x_anx_train, x_anx_test, y_anx_train, y_anx_test = train_test_split(X_anxiety[dim_red],
#                                                                     anxiety_os,
#                                                                     test_size=0.2,
#                                                                     random_state=42,
#                                                                     shuffle=True)


"""# SVM"""

from sklearn import svm

svm_classifier = svm.SVC(kernel='poly', C=1.0, probability=True)
svm_classifier.fit(x_anx_train, y_anx_train)
y_pred = svm_classifier.predict(x_anx_test)
accuracy = accuracy_score(y_anx_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, roc_curve

# Assuming y_true contains the true labels and y_pred contains the predicted labels

# Calculate F1 score
f1 = f1_score(y_anx_test, y_pred)
print("F1 score:", f1)

# Calculate precision
precision = precision_score(y_anx_test, y_pred)
print("Precision:", precision)

# Calculate recall
recall = recall_score(y_anx_test, y_pred)
print("Recall:", recall)

# Calculate ROC AUC score
y_pred_prob = svm_classifier.decision_function(x_anx_test)
y_pred_prob = 1 / (1 + np.exp(-y_pred_prob))
roc_auc = roc_auc_score(y_anx_test,
                        y_pred_prob)  # y_pred_prob is the predicted probability for positive class
print("ROC AUC score:", roc_auc)

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_anx_test,
                                 y_pred_prob)  # y_pred_prob is the predicted probability for positive class


def performance_eval(clf, X_test):
    y_pred = clf.predict(X_test)
    print(f'Accuracy : {accuracy_score(y_anx_test, y_pred)}\n')
    print('   ------------ Classification Report -----------')
    print(classification_report(y_anx_test, y_pred))
    print('   ------------ Confusion Matrix -------------- ')
    sns.set(rc={'figure.figsize': (5, 2)})
    sns.heatmap(confusion_matrix(y_anx_test, y_pred), annot=True, fmt='d')


from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

from sklearn import svm

svm_classifier = svm.SVC(kernel='poly', C=1.0)
svm_classifier.fit(x_anx_train, y_anx_train)
y_pred = svm_classifier.predict(x_anx_test)
accuracy = accuracy_score(y_anx_test, y_pred)
# print("Accuracy:", accuracy)
performance_eval(svm_classifier, x_anx_test)

"""# NN"""

x_anx_train, x_anx_test, y_anx_train, y_anx_test = train_test_split(
    X_anxiety[imp_feat_anx['features'].head(20).values],
    anxiety_os,
    test_size=0.2,
    random_state=42,
    shuffle=True)
x_anx_train, x_valid, y_anx_train, y_valid = train_test_split(x_anx_train, y_anx_train,
                                                              test_size=0.1, random_state=42)

x_anx_train.shape, x_valid.shape, x_anx_test.shape

import tensorflow as tf
from tensorflow import keras

model = keras.Sequential()
model.add(keras.layers.Dense(64, activation='relu', input_shape=(x_anx_train.shape[1],)))
model.add(keras.layers.Dense(64, activation='relu'))
model.add(keras.layers.Dense(1, activation='sigmoid'))

from tensorflow.keras.callbacks import EarlyStopping

# Define the early stopping criteria
early_stopping = EarlyStopping(monitor='val_loss', patience=5)

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_anx_train, y_anx_train, validation_data=(x_valid, y_valid), callbacks=[early_stopping],
          epochs=100)

loss, accuracy = model.evaluate(x_anx_test, y_anx_test)
print("Accuracy:", accuracy)
y_pred = model.predict(x_anx_test)

performance_eval(model, x_anx_test)

y_pred = np.round(y_pred)
y_pred

# Calculate F1 score
f1 = f1_score(y_anx_test, y_pred)
print("F1 score:", f1)

# Calculate precision
precision = precision_score(y_anx_test, y_pred)
print("Precision:", precision)

# Calculate recall
recall = recall_score(y_anx_test, y_pred)
print("Recall:", recall)

# Calculate ROC AUC score
# Convert predicted probabilities to binary predictions
y_pred = np.round(y_pred)

# Calculate ROC AUC score
roc_auc = roc_auc_score(y_anx_test, y_pred_prob)

# Calculate ROC accuracy
roc_accuracy = (y_anx_test == y_pred).mean()

print("ROC AUC score:", roc_auc)
print("ROC accuracy:", roc_accuracy)

roc_auc = roc_auc_score(y_anx_test,
                        y_pred_prob)  # y_pred_prob is the predicted probability for positive class
print("ROC AUC score:", roc_auc)

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_anx_test,
                                 y_pred_prob)  # y_pred_prob is the predicted probability for positive class

"""# Ensemble"""

# from sklearn.tree import DecisionTreeClassifier
# from sklearn.ensemble import RandomForestClassifier
# from xgboost import XGBClassifier
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score

# # Train decision tree model
# decision_tree = DecisionTreeClassifier()
# decision_tree.fit(x_anx_train, y_anx_train)

# # Train random forest model
# random_forest = RandomForestClassifier(n_estimators = 500)
# random_forest.fit(x_anx_train, y_anx_train)

# # Train XGBoost model
# xgboost = XGBClassifier()
# xgboost.fit(x_anx_train, y_anx_train)

# # Generate predictions from individual models
# dt_predictions = decision_tree.predict(x_anx_test)
# rf_predictions = random_forest.predict(x_anx_test)
# xgb_predictions = xgboost.predict(x_anx_test)

# ensemble_predictions = (dt_predictions + rf_predictions + xgb_predictions) / 3
# ensemble_predictions = np.round(ensemble_predictions).astype(int)

# ensemble_accuracy = accuracy_score(y_anx_test, ensemble_predictions)
# print("Ensemble Model Accuracy:", ensemble_accuracy)

# rf_predictions = random_forest.predict(x_anx_test)
# accuracy = accuracy_score(y_anx_test, rf_predictions)
# print("rf Accuracy:", accuracy)

# import numpy as np
# from sklearn.metrics import roc_curve, roc_auc_score
# import matplotlib.pyplot as plt


# y_pred = ensemble_predictions # Obtain predicted probabilities or class labels
# y_true = y_anx_test  # True labels of the test set

# fpr, tpr, thresholds = roc_curve(y_true, y_pred)  # Compute the false positive rate, true positive rate, and thresholds
# auc = roc_auc_score(y_true, y_pred)  # Calculate the area under the ROC curve


# plt.figure(figsize=(8, 6))
# plt.plot(fpr, tpr, label='ROC Curve (AUC = %0.2f)' % auc)
# plt.plot([0, 1], [0, 1], 'k--')  # Plot the diagonal line representing random guessing
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.05])
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('Receiver Operating Characteristic')
# plt.legend(loc='lower right')
# plt.show()

# roc_accuracy = roc_auc_score(y_true, y_pred)
# print(roc_accuracy)
# print(ensemble_accuracy)

"""# Ensemble 2"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

base_estimator = RandomForestClassifier(n_estimators=10, random_state=42)

# Initialize the AdaBoost classifier with the base estimator
adaboost_model = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)

# Train the AdaBoost model
adaboost_model.fit(x_anx_train, y_anx_train)

# Make predictions on the test set
y_pred = adaboost_model.predict(x_anx_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_anx_test, y_pred)
print("Accuracy:", accuracy)

performance_eval(adaboost_model, x_anx_test)

# Calculate F1 score
f1 = f1_score(y_anx_test, y_pred)
print("F1 score:", f1)

# Calculate precision
precision = precision_score(y_anx_test, y_pred)
print("Precision:", precision)

# Calculate recall
recall = recall_score(y_anx_test, y_pred)
print("Recall:", recall)

# Calculate ROC AUC score
# Convert predicted probabilities to binary predictions
y_pred = np.round(y_pred)

# Calculate ROC AUC score
roc_auc = roc_auc_score(y_anx_test, y_pred_prob)

# Calculate ROC accuracy
roc_accuracy = (y_anx_test == y_pred).mean()

print("ROC AUC score:", roc_auc)
print("ROC accuracy:", roc_accuracy)

roc_auc = roc_auc_score(y_anx_test,
                        y_pred_prob)  # y_pred_prob is the predicted probability for positive class
print("ROC AUC score:", roc_auc)

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_anx_test,
                                 y_pred_prob)  # y_pred_prob is the predicted probability for positive class

"""# Ensemble Tunning"""

# from sklearn.ensemble import RandomForestClassifier, VotingClassifier
# from sklearn.model_selection import train_test_split, GridSearchCV
# from sklearn.metrics import accuracy_score

# # Initialize individual base estimators
# estimators = [
#     ('rf1', RandomForestClassifier(random_state=42)),
#     ('rf2', RandomForestClassifier(random_state=42))
# ]

# # Initialize the ensemble model (Voting Classifier)
# ensemble_model = VotingClassifier(estimators=estimators, voting='hard')

# # Define the hyperparameters for tuning
# param_grid = {
#     'rf1__n_estimators': [50, 100, 200],
#     'rf1__max_depth': [None, 5, 10],
#     'rf1__min_samples_split': [2, 5, 10]
#     # 'rf2__n_estimators': [50, 100, 200],
#     # 'rf2__max_depth': [None, 5, 10],
#     # 'rf2__min_samples_split': [2, 5, 10]
# }

# # Perform grid search for hyperparameter tuning
# grid_search = GridSearchCV(estimator=ensemble_model, param_grid=param_grid, cv=5)
# grid_search.fit(x_anx_train, y_anx_train)

# # Retrieve the best model from grid search
# best_model = grid_search.best_estimator_

# # Make predictions on the test set using the best model
# y_pred = best_model.predict(x_anx_test)

# # Calculate the accuracy of the model
# accuracy = accuracy_score(y_anx_test, y_pred)
# print("Accuracy:", accuracy)

"""# LR"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(x_anx_train, y_anx_train)

y_pred = model.predict(x_anx_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_anx_test, y_pred)
print("Accuracy:", accuracy)

performance_eval(model, x_anx_test)

# Calculate F1 score
f1 = f1_score(y_anx_test, y_pred)
print("F1 score:", f1)

# Calculate precision
precision = precision_score(y_anx_test, y_pred)
print("Precision:", precision)

# Calculate recall
recall = recall_score(y_anx_test, y_pred)
print("Recall:", recall)

# Calculate ROC AUC score
# Convert predicted probabilities to binary predictions
y_pred = np.round(y_pred)

# Calculate ROC AUC score
roc_auc = roc_auc_score(y_anx_test, y_pred_prob)

# Calculate ROC accuracy
roc_accuracy = (y_anx_test == y_pred).mean()

print("ROC AUC score:", roc_auc)
print("ROC accuracy:", roc_accuracy)

roc_auc = roc_auc_score(y_anx_test,
                        y_pred_prob)  # y_pred_prob is the predicted probability for positive class
print("ROC AUC score:", roc_auc)

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_anx_test,
                                 y_pred_prob)  # y_pred_prob is the predicted probability for positive class

"""# XGBoost"""

import xgboost as xgb

xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',  # Objective function for binary classification
    n_estimators=100,  # Number of boosting rounds (trees)
    max_depth=3,  # Maximum depth of each tree
    learning_rate=0.1,  # Learning rate (shrinkage) to prevent overfitting
    random_state=42  # Random state for reproducibility
)

xgb_model.fit(x_anx_train, y_anx_train)

y_pred = xgb_model.predict(x_anx_test)

accuracy = accuracy_score(y_anx_test, y_pred)
print("Accuracy:", accuracy)

performance_eval(xgb_model, x_anx_test)

# Calculate F1 score
f1 = f1_score(y_anx_test, y_pred)
print("F1 score:", f1)

# Calculate precision
precision = precision_score(y_anx_test, y_pred)
print("Precision:", precision)

# Calculate recall
recall = recall_score(y_anx_test, y_pred)
print("Recall:", recall)

# Calculate ROC AUC score
# Convert predicted probabilities to binary predictions
y_pred = np.round(y_pred)

# Calculate ROC AUC score
roc_auc = roc_auc_score(y_anx_test, y_pred_prob)

# Calculate ROC accuracy
roc_accuracy = (y_anx_test == y_pred).mean()

print("ROC AUC score:", roc_auc)
print("ROC accuracy:", roc_accuracy)

roc_auc = roc_auc_score(y_anx_test,
                        y_pred_prob)  # y_pred_prob is the predicted probability for positive class
print("ROC AUC score:", roc_auc)

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_anx_test,
                                 y_pred_prob)  # y_pred_prob is the predicted probability for positive class